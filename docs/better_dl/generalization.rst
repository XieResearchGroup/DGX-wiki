Better Deep Learning
***************************

Better Generalization
==================

penalize the large weights with weight regularization
-----------------
sparse representation with activity regularization
-----------------
1. large activations may indicate an over-fit model
2. there is a tension between the expressiveness and the generalization of the learned features
3. encourage small activations with additional penalty
4. track activation mean value

force small weights with weight constraints
-----------------

decouple layers with dropout
-----------------

promote robustness with Noise
-----------------

halt training at the right time with early stopping
-----------------
